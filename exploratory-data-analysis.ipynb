{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an EDA for this project. I have documented my thought processes and decisions in the notebook. Some contributions include:\n",
    "- Scalable data collection and wrangling methods\n",
    "- Some exploratory data visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to EDA is data collection. We will use copyright free books from The Project Gutenberg to do our analysis. We will query the books using gutendex.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get book content from Project Gutenberg\n",
    "def get_book_content(book_number: int) -> str:\n",
    "    gutendex_url = f\"https://gutendex.com/books/{book_number}/\"\n",
    "    try:\n",
    "        response = requests.get(gutendex_url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        book_url = data[\"formats\"][\"text/plain\"]\n",
    "\n",
    "        try:\n",
    "            response = requests.get(book_url)\n",
    "            response.raise_for_status()\n",
    "            text = response.text\n",
    "            return text.replace(\"\\r\\n\", \" \")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"Error retrieving book content:\", str(e))\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error retrieving book information:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try parsing sentences from The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson. We'll use the SpaCy's parser to parse the sentences from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_book_content(43)\n",
    "\n",
    "doc = nlp(content)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "pprint(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from eyeballing the list of sentences, it looks like the parser has done a good job. Although, it looks like there are some unnecessary sentences in the list, such as The Project Gutenberg header and footer. However, before we get into cleaning up the data, let's try to understand the data a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This book has {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the longest sentence in the book? What about the shortest sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The longest sentence is:\")\n",
    "pprint(max(sentences, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shortest sentence is: {min(sentences, key=len)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon verifying the output from the actual book on The Project Gutenberg (https://www.gutenberg.org/cache/epub/43/pg43.txt), it looks like there may be some outliers in the data (unwanted text). For example, the shortest sentence is a single character, which is probably from a bullet point list from The Project Gutenberg footer. Fortunately, The Project Gutenberg makes this easy to do, as they provide markers such as:\n",
    "\n",
    "> *** START OF THE PROJECT GUTENBERG EBOOK\n",
    "\n",
    "> *** END OF THE PROJECT GUTENBERG EBOOK\n",
    "\n",
    "Unfortunately, we will have to manually remove the outliers before we feed the data into the parser. Let's try to remove the outliers and parse the sentences again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_marker(text: str) -> str:\n",
    "    start_marker = \"***\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "    # Remove everything before the second occurrence of the start marker\n",
    "    start_index = text.find(start_marker, text.find(start_marker) + 1)\n",
    "    # Remove everything after the first occurrence of the end marker\n",
    "    end_index = text.find(end_marker)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        text = text[start_index + len(start_marker) : end_index].strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = remove_marker(content)\n",
    "\n",
    "doc = nlp(content)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "pprint(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! To verify, let's check the number of sentences again and look at the shortest sentence in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This book has {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shortest sentence is: {min(sentences, key=len)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, what went wrong here? Surely the last sentence can't just be \"DR.\"? Let's take a look at the sentence before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_sentence = min(sentences, key=len)\n",
    "shortest_sentence_index = sentences.index(shortest_sentence)\n",
    "\n",
    "sentence_before = sentences[shortest_sentence_index - 1]\n",
    "sentence_after = sentences[shortest_sentence_index + 1]\n",
    "\n",
    "print(f\"Before : {sentence_before}\")\n",
    "print(f\"Current: {shortest_sentence}\")\n",
    "print(f\"After  : {sentence_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like SpaCy parser has interpreted the period after \"DR\" as the end of the sentence. This may have to do with the fact that [SpaCy uses a non-monotonic arc-eager transition-system](https://spacy.io/api/dependencyparser/), a form of rule-based method to parse sentences. It may also have to do with the fact that we are using a less accurate model (en_core_web_sm) to parse the sentences. Nonetheless, I think we sanitized the data enough to move on to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to separate out the sentences with subordinating conjunctions. We will continue to use SpaCy's dependency parser to do this. Specifically, SpaCy's part-of-speech (POS) tagging allows us to identify subordinating conjunctions as the tag \"SCONJ\". Let's try to identify the sentences with subordinating conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_sconj = [sent for sent in sentences if any(token.pos_ == \"SCONJ\" for token in nlp(sent))]\n",
    "pprint(sentences_with_sconj[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sentences did we identify with subordinating conjunctions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(sentences_with_sconj)} sentences with a subordinating conjunction, out of {len(sentences)} sentences in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizing one of the sentences with subordinating conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(sentences_with_sconj[455])\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"distance\": 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some simple visualization. What are the most common subordinating conjunctions in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subordinating_conjunctions = []\n",
    "\n",
    "for sentence in sentences_with_sconj:\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"SCONJ\":\n",
    "            subordinating_conjunctions.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(subordinating_conjunctions, columns=[\"sconj\"])\n",
    "subordinating_conjunctions_df = (\n",
    "    temp_df.groupby(\"sconj\", as_index=False)\n",
    "    .size()\n",
    "    .sort_values(\"size\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "subordinating_conjunctions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    subordinating_conjunctions_df,\n",
    "    x=\"sconj\",\n",
    "    y=\"size\",\n",
    "    title=\"Most common subordinating conjunctions\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is somewhat surprising to me. I did not expect that \"that\" would be the most common subordinating conjunction in the book. I had expected \"because\" to be more common in comparison to other subordinating conjunctions used in the book. However, subordinating conjunction alone might not be as interesting as the words that follow the subordinating conjunctions. Let's try to look at the rest of the sentence after the subordinating conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "\n",
    "for sentence in sentences_with_sconj:\n",
    "    doc = nlp(sentence)\n",
    "    sconj_index = next((i for i, token in enumerate(doc) if token.pos_ == \"SCONJ\"), None)\n",
    "    if sconj_index is not None:\n",
    "        phrase = \" \".join([token.text for token in doc[sconj_index:]])\n",
    "        phrases.append(phrase)\n",
    "\n",
    "pprint(phrases[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting! It looks like the words that follow the subordinating conjunctions explain the reason for some action in some way. For example, if my sentence with subordinating conjunction is:\n",
    "\n",
    "> Will Hyde die upon the scaffold?\n",
    "\n",
    "The words up to the subordinating conjunction \"upon\" are incomplete thoughts. Imagine if you were to read the sentence up to \"upon\":\n",
    "\n",
    "> Will Hyde die\n",
    "\n",
    "You would be left with many possibilities to complete the sentence. However, when you add the word \"upon\" the sentence, it sorts of begs the question \"upon what?\":\n",
    "\n",
    "> Will Hyde die upon _what?_\n",
    "\n",
    "You could imagine that the writer could potentially complete the sentence (in this case, Robert Louis Stevenson) with something like:\n",
    "\n",
    "> Will Hyde die upon _the scaffold?_\n",
    "\n",
    "This leaves me with several questions:\n",
    "- What are some good examples of sentences that uses subordinating conjunctions to explain the reason for some action? \n",
    "- What about sentences that uses subordinating conjunctions in the beginning of the sentence, like \"Upon the scaffold, will Hyde die?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
